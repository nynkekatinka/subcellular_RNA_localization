{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from math import comb\n",
    "import gc\n",
    "import itertools\n",
    "from dataprep import *\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version that was able to run in 120 minutes. The test_permutation() took 6 seconds with multithreading and 30 seconds without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import comb\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "def test_permutation(pattern, control, n_permutations: int = 9999, return_distances: bool = False):\n",
    "    ''' pattern and control are subsets of adata.obsm['latent'].\n",
    "    '''\n",
    "    combined = np.concatenate([pattern, control])\n",
    "    distance_matrix = squareform(pdist(combined, metric='cityblock'))\n",
    "    len_combined = len(combined)\n",
    "    num_pattern = len(pattern)\n",
    "    observed_statistic = chamfer_L1_distance(distance_matrix, list(range(len_combined)))\n",
    "\n",
    "    if num_pattern < 15:\n",
    "        total_permutations = comb(len_combined, num_pattern)\n",
    "        if n_permutations > total_permutations:\n",
    "            exact_test = True\n",
    "            n_permutations = int(total_permutations)\n",
    "        else:\n",
    "            exact_test = False\n",
    "    else:\n",
    "        exact_test = False\n",
    "\n",
    "    index_lists = np.apply_along_axis(np.random.permutation, 1, np.tile(list(range(len_combined)), (n_permutations, 1)))\n",
    "\n",
    "    chamfer_distances = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(chamfer_L1_distance, distance_matrix, index_list) for index_list in index_lists]\n",
    "        for future in as_completed(futures):\n",
    "            chamfer_distances.append(future.result())\n",
    "\n",
    "    chamfer_distances = np.array(chamfer_distances)\n",
    "\n",
    "    eps = (0 if not np.issubdtype(observed_statistic.dtype, np.inexact)\n",
    "           else np.finfo(observed_statistic.dtype).eps * 100)\n",
    "    gamma = np.abs(eps * observed_statistic)\n",
    "    cmps_greater = chamfer_distances >= observed_statistic - gamma\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum() + adjustment) / (n_permutations + adjustment)\n",
    "    p_value = pvalues_greater\n",
    "\n",
    "    if return_distances:\n",
    "        return p_value, observed_statistic, chamfer_distances\n",
    "    else:\n",
    "        return p_value\n",
    "\n",
    "def compute_power_permutation(params):\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        # Given that random patterns have only 1442 cells simulated, we don't calculate the power for these above 1400 so that we don't need to sample with replacement\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "    \n",
    "        for i in range(1000):\n",
    "            # sample new gene. No random seed so that every time a different \"gene\" is sampled.\n",
    "            pattern = subset_power_analysis(adata_test, mixed_patterns = True, pattern_strength= strength, rna_count = count, sample_size = sample, random_seed=False)\n",
    "            control = subset_power_analysis(adata_test, pattern = 'random', mixed_patterns = False, rna_count = count, sample_size = sample, random_seed=False)\n",
    "\n",
    "            # Calculate null distribution and pvalue\n",
    "            pvalue = test_permutation(pattern.obsm[\"latent\"], control.obsm[\"latent\"], n_permutations=9999, return_distances = False)\n",
    "            \n",
    "            critical_value = 0.05\n",
    "            adjusted_critical_value = critical_value / 5000\n",
    "            if pvalue < critical_value:\n",
    "                significant_count += 1\n",
    "            if pvalue < adjusted_critical_value:\n",
    "                bonferroni_count += 1\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count/1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count/1000)\n",
    "\n",
    "        return result, bonferroni_result\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(filename='permute.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "    logging.info('Loading adata')\n",
    "\n",
    "    adata_split_cellID = sc.read_h5ad(\"/media/gambino/students_workdir/nynke/new_model_with_cell_id_left_out_custom_nynke_panel_simulated_embeddings_adata.h5ad\")\n",
    "    adata_split_cellID = initialize_adata(adata_split_cellID)\n",
    "    adata_test = adata_split_cellID[adata_split_cellID.obs['cell_id'].isin(adata_split_cellID.uns['test_cellIDs'])]\n",
    "\n",
    "    strengths = ['strong', 'intermediate', 'low']\n",
    "    counts = adata_test.obs['rna_count'].unique()\n",
    "    #samples = [5, 9, 15, 27, 46, 81, 142, 247, 432, 753, 1315, 2297, 4009, 7000]\n",
    "    samples = [1315, 2297, 4009, 7000]\n",
    "\n",
    "    # Create a list of all combinations of strength, count, and sample\n",
    "    combinations = [(strength, count, sample) for strength in strengths for count in counts for sample in samples]\n",
    "\n",
    "    # Set the start method to 'spawn'\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    # Create a multiprocessing pool and compute the power for each combination\n",
    "    with Pool(20) as p:\n",
    "        results, bonferroni_result = p.map(compute_power_permutation, combinations)\n",
    "\n",
    "    # in case want to do single processing:\n",
    "    # results = [compute_power_permutation(combination) for combination in combinations]\n",
    "\n",
    "    # Convert the results to a dictionary\n",
    "    power_results = dict(results)\n",
    "    bonferroni_power_results = dict(bonferroni_result)\n",
    "\n",
    "    path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_uncorrected.pkl\"\n",
    "    bonferroni_path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_bonferroni.pkl\"\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(power_results, f)\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(bonferroni_path, 'wb') as f:\n",
    "        pickle.dump(bonferroni_power_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import comb\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import logging\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "from multiprocessing import Pool, set_start_method\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "def test_permutation(pattern, control, n_permutations: int = 9999, exact_test: bool = False, return_distances: bool = False):\n",
    "    ''' pattern and control are subsets of adata.obsm['latent'].\n",
    "    '''\n",
    "    combined = np.concatenate([pattern, control])\n",
    "    distance_matrix = squareform(pdist(combined, metric='cityblock'))\n",
    "    len_combined = len(combined)\n",
    "    num_pattern = len(pattern)\n",
    "    observed_statistic = chamfer_L1_distance(distance_matrix, list(range(len_combined)))\n",
    "\n",
    "    index_lists = np.apply_along_axis(np.random.permutation, 1, np.tile(list(range(len_combined)), (n_permutations, 1)))\n",
    "\n",
    "    chamfer_distances = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(chamfer_L1_distance, distance_matrix, index_list) for index_list in index_lists]\n",
    "        for future in as_completed(futures):\n",
    "            chamfer_distances.append(future.result())\n",
    "\n",
    "    chamfer_distances = np.array(chamfer_distances)\n",
    "\n",
    "    eps = (0 if not np.issubdtype(observed_statistic.dtype, np.inexact)\n",
    "           else np.finfo(observed_statistic.dtype).eps * 100)\n",
    "    gamma = np.abs(eps * observed_statistic)\n",
    "    cmps_greater = chamfer_distances >= observed_statistic - gamma\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum() + adjustment) / (n_permutations + adjustment)\n",
    "    p_value = pvalues_greater\n",
    "\n",
    "    if return_distances:\n",
    "        return p_value, observed_statistic, chamfer_distances\n",
    "    else:\n",
    "        return p_value\n",
    "\n",
    "def compute_power_permutation(params):\n",
    "    strength, count, sample = params\n",
    "    significant_count = 0\n",
    "    bonferroni_count = 0\n",
    "\n",
    "    # Given that random patterns have only 1442 cells simulated, we don't calculate the power for these above 1400 so that we don't need to sample with replacement\n",
    "    if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "        with open('/media/gambino/students_workdir/nynke/blurry/results.txt', 'a') as f:\n",
    "            f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "        return (f'{strength}_{count}_{sample}', -1)\n",
    "    \n",
    "    # Count max number of permutations with Combination rule nCr, where r is the pattern size\n",
    "    if sample < 15:\n",
    "        total_permutations = comb(sample*2, sample) # built in implementation of nCr rule.\n",
    "        # Adjust n_permutations if it's larger than total_permutations\n",
    "        if n_permutations > total_permutations:\n",
    "            exact_test = True\n",
    "            n_permutations = int(total_permutations)\n",
    "        else:\n",
    "            n_permutations = 9999\n",
    "            exact_test = False\n",
    "    else:\n",
    "        # If num_pattern is 15, the total combinations are 1.5e8, which already is much larger than 9999. So we skip calculating the factorials for 15+ to save compute time. \n",
    "        n_permutations = 9999\n",
    "        exact_test = False\n",
    "\n",
    "    try:\n",
    "        pvalues = np.zeros(1000)\n",
    "        for i in range(1000):\n",
    "            # sample new gene. No random seed so that every time a different \"gene\" is sampled.\n",
    "            pattern = subset_power_analysis(adata_test, mixed_patterns = True, pattern_strength= strength, rna_count = count, sample_size = sample, random_seed=False)\n",
    "            control = subset_power_analysis(adata_test, pattern = 'random', mixed_patterns = False, rna_count = count, sample_size = sample, random_seed=False)\n",
    "\n",
    "            # Calculate null distribution and pvalue\n",
    "            pvalues[i] = test_permutation(pattern.obsm[\"latent\"], control.obsm[\"latent\"], n_permutations=9999, exact_test = exact_test, return_distances = False)\n",
    "            \n",
    "        critical_value = 0.05\n",
    "        adjusted_critical_value = critical_value / 5000\n",
    "        significant_count = np.sum(pvalues < critical_value)\n",
    "        bonferroni_count = np.sum(pvalues < adjusted_critical_value)\n",
    "\n",
    "        with open('/media/gambino/students_workdir/nynke/blurry/results.txt', 'a') as f:\n",
    "            f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count/1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count/1000)\n",
    "\n",
    "        return result, bonferroni_result\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(filename='permute.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "    \n",
    "    logging.info('Loading adata')\n",
    "    adata_split_cellID = sc.read_h5ad(\"/media/gambino/students_workdir/nynke/new_model_with_cell_id_left_out_custom_nynke_panel_simulated_embeddings_adata.h5ad\")\n",
    "    adata_split_cellID = initialize_adata(adata_split_cellID)\n",
    "    adata_test = adata_split_cellID[adata_split_cellID.obs['cell_id'].isin(adata_split_cellID.uns['test_cellIDs'])]\n",
    "\n",
    "    strengths = ['strong', 'intermediate', 'low']\n",
    "    counts = adata_test.obs['rna_count'].unique()\n",
    "    #samples = [5, 9, 15, 27, 46, 81, 142, 247, 432, 753, 1315, 2297, 4009, 7000]\n",
    "    samples = [1315, 2297, 4009, 7000]\n",
    "\n",
    "    # Create a list of all combinations of strength, count, and sample\n",
    "    combinations = [(strength, count, sample) for strength in strengths for count in counts for sample in samples]\n",
    "\n",
    "    # Set the start method to 'spawn'\n",
    "    set_start_method('spawn', force=True)\n",
    "\n",
    "    # Create a multiprocessing pool and compute the power for each combination\n",
    "    with Pool(20) as p:\n",
    "        results = p.map(compute_power_permutation, combinations)\n",
    "\n",
    "    # Convert the results to a dictionary\n",
    "    power_results = {result[0]: result[1] for result, _ in results if result}\n",
    "    bonferroni_power_results = {bonferroni_result[0]: bonferroni_result[1] for _, bonferroni_result in results if bonferroni_result}\n",
    "\n",
    "    path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_uncorrected.pkl\"\n",
    "    bonferroni_path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_bonferroni.pkl\"\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(power_results, f)\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(bonferroni_path, 'wb') as f:\n",
    "        pickle.dump(bonferroni_power_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[index_lists[:, :len_pattern], :][:, :, index_lists[:, len_pattern:]], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrix[index_lists[:, len_pattern:], :][:, :, index_lists[:, :len_pattern]], axis=2)\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "\n",
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of permutations.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 2D numpy array representing the pairwise distance matrix.\n",
    "        index_lists (ndarray): A 2D numpy array where each row represents a permutation of indices.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for each permutation.\n",
    "    \"\"\"\n",
    "    len_combined = distance_matrix.shape[0]\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "\n",
    "    # Create a 3D index array for pattern to control distances\n",
    "    idx1 = np.arange(index_lists.shape[0])[:, None, None]\n",
    "    idx2 = index_lists[:, :len_pattern][:, :, None]\n",
    "    idx3 = index_lists[:, len_pattern:][:, None, :]\n",
    "    \n",
    "    distances_1_to_2 = np.min(distance_matrix[idx1, idx2, idx3], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrix[idx1, idx3, idx2], axis=2)\n",
    "    \n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "\n",
    "def permutation_test_batch(patterns, controls, n_permutations=9999):\n",
    "    combined = np.concatenate((patterns, controls), axis=1)\n",
    "    num_patterns = patterns.shape[1]\n",
    "    len_combined = combined.shape[1]\n",
    "\n",
    "    observed_statistics = []\n",
    "    for pattern, control in zip(patterns, controls):\n",
    "        combined_pc = np.concatenate((pattern, control))\n",
    "        distance_matrix = squareform(pdist(combined_pc, metric='cityblock'))\n",
    "        observed_statistic = chamfer_L1_distance(distance_matrix, np.arange(len_combined))\n",
    "        observed_statistics.append(observed_statistic)\n",
    "    \n",
    "    observed_statistics = np.array(observed_statistics)\n",
    "\n",
    "    index_lists = np.array([np.random.permutation(len_combined) for _ in range(n_permutations)])\n",
    "    chamfer_distances = []\n",
    "    for combined_pc in combined:\n",
    "        distance_matrix = squareform(pdist(combined_pc, metric='cityblock'))\n",
    "        chamfer_distances.append(chamfer_L1_distance_batch(distance_matrix, index_lists))\n",
    "\n",
    "    chamfer_distances = np.array(chamfer_distances)\n",
    "\n",
    "    eps = np.finfo(observed_statistics.dtype).eps * 100\n",
    "    gamma = np.abs(eps * observed_statistics)\n",
    "    cmps_greater = chamfer_distances >= observed_statistics[:, None] - gamma[:, None]\n",
    "\n",
    "    pvalues_greater = (cmps_greater.sum(axis=1) + 1) / (n_permutations + 1)\n",
    "    return pvalues_greater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power_permutation_batch(params):\n",
    "    logging.info('Starting compute_power_permutation with params: %s', params)\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "        \n",
    "        patterns = []\n",
    "        controls = []\n",
    "\n",
    "        for i in range(1000):\n",
    "            pattern = subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False)\n",
    "            control = subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False)\n",
    "            patterns.append(pattern.obsm[\"latent\"])\n",
    "            controls.append(control.obsm[\"latent\"])\n",
    "\n",
    "        patterns = np.array(patterns)\n",
    "        controls = np.array(controls)\n",
    "\n",
    "        pvalues = permutation_test_batch(patterns, controls, n_permutations=9999)\n",
    "\n",
    "        critical_value = 0.05\n",
    "        adjusted_critical_value = critical_value / 5000\n",
    "        significant_count = np.sum(pvalues < critical_value)\n",
    "        bonferroni_count = np.sum(pvalues < adjusted_critical_value)\n",
    "\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count / 1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count / 1000)\n",
    "        \n",
    "        with open('/media/gambino/students_workdir/nynke/blurry/results.txt', 'a') as f:\n",
    "            f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "        \n",
    "        logging.info('Finished compute_power_permutation with params: %s', params)\n",
    "        return result, bonferroni_result\n",
    "\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_split_cellID = sc.read_h5ad(\"/media/gambino/students_workdir/nynke/new_model_with_cell_id_left_out_custom_nynke_panel_simulated_embeddings_adata.h5ad\")\n",
    "adata_split_cellID = initialize_adata(adata_split_cellID)\n",
    "adata_test = adata_split_cellID[adata_split_cellID.obs['cell_id'].isin(adata_split_cellID.uns['test_cellIDs'])]\n",
    "combination = ('low', '10-30', 753)\n",
    "strength, count, sample = combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 329349 × 15\n",
       "    obs: 'pattern', 'random_or_pattern', 'n_spots', 'n_spots_interval', 'cell_id', 'genes', 'rotation', 'rotation_interval', 'blur', 'prop', 'prop_interval', 'corresponding_dapis', 'train_or_val', 'original_image_paths', 'pattern_strength', 'rna_count'\n",
       "    uns: 'test_cellIDs', 'train_cellIDs'\n",
       "    obsm: 'latent'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('strong', '0-10', 1315), ('strong', '0-10', 2297), ('strong', '0-10', 4009), ('strong', '10-30', 1315), ('strong', '10-30', 2297), ('strong', '10-30', 4009), ('strong', '30-60', 1315), ('strong', '30-60', 2297), ('strong', '30-60', 4009), ('strong', '60-100', 1315), ('strong', '60-100', 2297), ('strong', '60-100', 4009), ('strong', '100+', 1315), ('strong', '100+', 2297), ('strong', '100+', 4009), ('intermediate', '0-10', 1315), ('intermediate', '0-10', 2297), ('intermediate', '0-10', 4009), ('intermediate', '10-30', 1315), ('intermediate', '10-30', 2297), ('intermediate', '10-30', 4009), ('intermediate', '30-60', 1315), ('intermediate', '30-60', 2297), ('intermediate', '30-60', 4009), ('intermediate', '60-100', 1315), ('intermediate', '60-100', 2297), ('intermediate', '60-100', 4009), ('intermediate', '100+', 1315), ('intermediate', '100+', 2297), ('intermediate', '100+', 4009), ('low', '0-10', 1315), ('low', '0-10', 2297), ('low', '0-10', 4009), ('low', '10-30', 1315), ('low', '10-30', 2297), ('low', '10-30', 4009), ('low', '30-60', 1315), ('low', '30-60', 2297), ('low', '30-60', 4009), ('low', '60-100', 1315), ('low', '60-100', 2297), ('low', '60-100', 4009), ('low', '100+', 1315), ('low', '100+', 2297), ('low', '100+', 4009)]\n",
      "[('strong', '0-10', 1315), ('strong', '0-10', 2297), ('strong', '0-10', 4009), ('strong', '10-30', 1315), ('strong', '10-30', 2297), ('strong', '10-30', 4009), ('strong', '30-60', 1315), ('strong', '30-60', 2297), ('strong', '30-60', 4009), ('strong', '60-100', 1315), ('strong', '60-100', 2297), ('strong', '60-100', 4009), ('strong', '100+', 1315), ('strong', '100+', 2297), ('strong', '100+', 4009), ('intermediate', '0-10', 1315), ('intermediate', '0-10', 2297), ('intermediate', '0-10', 4009), ('intermediate', '10-30', 1315), ('intermediate', '10-30', 2297), ('intermediate', '10-30', 4009), ('intermediate', '30-60', 1315), ('intermediate', '30-60', 2297), ('intermediate', '30-60', 4009), ('intermediate', '60-100', 1315), ('intermediate', '60-100', 2297), ('intermediate', '60-100', 4009), ('intermediate', '100+', 1315), ('intermediate', '100+', 2297), ('intermediate', '100+', 4009), ('low', '0-10', 1315), ('low', '0-10', 2297), ('low', '0-10', 4009), ('low', '10-30', 1315), ('low', '10-30', 2297), ('low', '10-30', 4009), ('low', '30-60', 1315), ('low', '30-60', 2297), ('low', '30-60', 4009), ('low', '60-100', 1315), ('low', '60-100', 2297), ('low', '60-100', 4009), ('low', '100+', 1315), ('low', '100+', 2297), ('low', '100+', 4009), ('low', '0-10', 753)]\n"
     ]
    }
   ],
   "source": [
    "strengths = ['strong', 'intermediate', 'low']\n",
    "counts = ['0-10', '10-30', '30-60', '60-100','100+']\n",
    "#samples = [5, 9, 15, 27, 46, 81, 142, 247, 432, 753, 1315, 2297, 4009, 7000]\n",
    "samples = [1315, 2297, 4009] # , 7000]\n",
    "\n",
    "# Create a list of all combinations of strength, count, and sample\n",
    "combinations = [(strength, count, sample) for strength in strengths for count in counts for sample in samples]\n",
    "print(combinations)\n",
    "combinations.append(('low', '0-10', 753))\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, Manager\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from math import comb\n",
    "from dataprep import *\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "\n",
    "\n",
    "def compute_power_permutation(params, adata_test):\n",
    "    logging.info('Starting compute_power_permutation with params: %s', params)\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "        \n",
    "        # Generate 1000 samples of pattern and control\n",
    "        patterns = np.array([subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "        controls = np.array([subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "\n",
    "        # Count max number of permutations with Combination rule nCr, where r is the pattern size\n",
    "        if sample < 15:\n",
    "            total_permutations = comb(sample*2, sample) # built in implementation of nCr rule.\n",
    "            # Adjust n_permutations if it's larger than total_permutations\n",
    "            if n_permutations > total_permutations:\n",
    "                exact_test = True\n",
    "                n_permutations = int(total_permutations)\n",
    "            else:\n",
    "                n_permutations = 9999\n",
    "                exact_test = False\n",
    "        else:\n",
    "            # If num_pattern is 15, the total combinations are 1.5e8, which already is much larger than 9999. So we skip calculating the factorials for 15+ to save compute time. \n",
    "            n_permutations = 9999\n",
    "            exact_test = False\n",
    "\n",
    "        pvalues = permutation_test_batch(patterns, controls, n_permutations=n_permutations, exact_test=exact_test)\n",
    "\n",
    "        critical_value = 0.05\n",
    "        adjusted_critical_value = critical_value / 5000\n",
    "        significant_count = np.sum(pvalues < critical_value)\n",
    "        bonferroni_count = np.sum(pvalues < adjusted_critical_value)\n",
    "\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count / 1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count / 1000)\n",
    "        \n",
    "        with open('/media/gambino/students_workdir/nynke/blurry/results_permutation.txt', 'a') as f:\n",
    "            f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "        \n",
    "        logging.info('Finished compute_power_permutation with params: %s', params)\n",
    "        return result, bonferroni_result\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "def chamfer_L1_distance_batch(distance_matrices, index_list):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of distance matrices\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 3D numpy array representing the pairwise distance matrix for many samples.\n",
    "        index_lists (ndarray): A 1D numpy array represents a permutation of indices.\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for all samples.\n",
    "    \"\"\"\n",
    "    len_pattern = index_list.shape[0] // 2\n",
    "    n_power_iterations=distance_matrices.shape[0]\n",
    "    # Create a 3D index array for pattern to control distances\n",
    "    idx2 = index_list[:len_pattern]\n",
    "    idx3 = index_list[len_pattern:]\n",
    "    distances_1_to_2 = np.min(distance_matrices[np.ix_(np.arange(n_power_iterations), idx2, idx3)], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrices[np.ix_(np.arange(n_power_iterations), idx3, idx2)], axis=2)\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "\n",
    "def permutation_test_batch(patterns, controls, n_permutations=9999, exact_test=False):\n",
    "    combined = np.concatenate((patterns, controls), axis=1)\n",
    "    len_combined = combined.shape[1]\n",
    "    indices_list = np.arange(len_combined)\n",
    "\n",
    "    distance_matrices = np.array([squareform(pdist(combined[i], metric='cityblock')) for i in range(1000)])\n",
    "\n",
    "    # Compute observed statistics for all samples\n",
    "    observed_statistics = np.array([chamfer_L1_distance_batch(distance_matrices, indices_list)])\n",
    "    \n",
    "    # Generate index permutations\n",
    "    index_lists = np.array([np.random.permutation(len_combined) for _ in range(n_permutations)])\n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples using multithreading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        chamfer_distances = np.array(list(executor.map(lambda index_list: chamfer_L1_distance_batch(distance_matrices, index_list), index_lists)))\n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples\n",
    "    #chamfer_distances = np.array([chamfer_L1_distance_batch(distance_matrices, index_list) for index_list in index_lists])\n",
    "\n",
    "\n",
    "    # Calculate p-values (one-sided test cause only interested in larger distances than H0)\n",
    "    eps = np.finfo(observed_statistics.dtype).eps * 100\n",
    "    gamma = np.abs(eps * observed_statistics)\n",
    "    cmps_greater = chamfer_distances >= observed_statistics[:, None] - gamma[:, None]\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum(axis=1) + adjustment) / (n_permutations + adjustment)\n",
    "    return pvalues_greater\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(filename='permute.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "    logging.info('Loading adata')\n",
    "\n",
    "    adata_split_cellID = sc.read_h5ad(\"/media/gambino/students_workdir/nynke/new_model_with_cell_id_left_out_custom_nynke_panel_simulated_embeddings_adata.h5ad\")\n",
    "    adata_split_cellID = initialize_adata(adata_split_cellID)\n",
    "    adata_test = adata_split_cellID[adata_split_cellID.obs['cell_id'].isin(adata_split_cellID.uns['test_cellIDs'])]\n",
    "    params=('low', '10-30', 753)\n",
    "    logging.info('Adata loaded')\n",
    "    result, bonferroni_result = compute_power_permutation(params, adata_test)\n",
    "    logging.info('Result: %s', result)\n",
    "    print(\"hooray we're done!\")\n",
    "    '''\n",
    "\n",
    "    strengths = ['strong', 'intermediate', 'low']\n",
    "    counts = adata_test.obs['rna_count'].unique()\n",
    "    #samples = [5, 9, 15, 27, 46, 81, 142, 247, 432, 753, 1315, 2297, 4009, 7000]\n",
    "    samples = [1315, 2297, 4009, 7000]\n",
    "\n",
    "    # Create a list of all combinations of strength, count, and sample\n",
    "    combinations = [(strength, count, sample) for strength in strengths for count in counts for sample in samples]\n",
    "\n",
    "    # Set the start method to 'spawn'\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "    # Create a multiprocessing pool and compute the power for each combination\n",
    "    with Manager() as manager:\n",
    "        shared_adata_test = manager.list([adata_test])\n",
    "        with Pool(20) as p:\n",
    "            results, bonferroni_result = p.starmap(compute_power_permutation, [(params, shared_adata_test) for params in combinations])\n",
    "\n",
    "    # in case want to do single processing:\n",
    "    # results = [compute_power_permutation(combination) for combination in combinations]\n",
    "\n",
    "    # Convert the results to a dictionary\n",
    "    power_results = dict(results)\n",
    "    bonferroni_power_results = dict(bonferroni_result)\n",
    "\n",
    "    path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_uncorrected.pkl\"\n",
    "    bonferroni_path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_bonferroni.pkl\"\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(power_results, f)\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(bonferroni_path, 'wb') as f:\n",
    "        pickle.dump(bonferroni_power_results, f)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set up logging\n",
    "    logging.basicConfig(filename='permute.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)\n",
    "    logging.info('Loading adata')\n",
    "\n",
    "    adata_split_cellID = sc.read_h5ad(\"/media/gambino/students_workdir/nynke/new_model_with_cell_id_left_out_custom_nynke_panel_simulated_embeddings_adata.h5ad\")\n",
    "    adata_split_cellID = initialize_adata(adata_split_cellID)\n",
    "    adata_test = adata_split_cellID[adata_split_cellID.obs['cell_id'].isin(adata_split_cellID.uns['test_cellIDs'])]\n",
    "\n",
    "    strengths = ['strong', 'intermediate', 'low']\n",
    "    counts = adata_test.obs['rna_count'].unique()\n",
    "    #samples = [5, 9, 15, 27, 46, 81, 142, 247, 432, 753, 1315, 2297, 4009, 7000]\n",
    "    samples = [1315, 2297, 4009, 7000]\n",
    "\n",
    "    # Create a list of all combinations of strength, count, and sample\n",
    "    combinations = [(strength, count, sample) for strength in strengths for count in counts for sample in samples]\n",
    "\n",
    "    # Set the start method to 'spawn'\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    # Create a multiprocessing pool and compute the power for each combination\n",
    "    with Pool(20) as p:\n",
    "        results, bonferroni_result = p.map(compute_power_permutation, combinations)\n",
    "\n",
    "    # in case want to do single processing:\n",
    "    # results = [compute_power_permutation(combination) for combination in combinations]\n",
    "\n",
    "    # Convert the results to a dictionary\n",
    "    power_results = dict(results)\n",
    "    bonferroni_power_results = dict(bonferroni_result)\n",
    "\n",
    "    path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_uncorrected.pkl\"\n",
    "    bonferroni_path = \"temp_objects/power_analysis_permutationLatent_to7000_logscale_bonferroni.pkl\"\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(power_results, f)\n",
    "\n",
    "    # Open the file in write-binary mode and dump the object\n",
    "    with open(bonferroni_path, 'wb') as f:\n",
    "        pickle.dump(bonferroni_power_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = np.array([subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(10)])\n",
    "controls = np.array([subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(10)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "controls = []\n",
    "\n",
    "for i in range(10):\n",
    "    pattern = subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False)\n",
    "    control = subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False)\n",
    "    patterns.append(pattern.obsm[\"latent\"])\n",
    "    controls.append(control.obsm[\"latent\"])\n",
    "\n",
    "patterns = np.array(patterns)\n",
    "controls = np.array(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 753, 15), (10, 753, 15))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns.shape, controls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1506, 15), (1506,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = np.concatenate((patterns, controls), axis=1)\n",
    "len_combined = combined.shape[1]\n",
    "indices_list = np.arange(len_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1506, 1506)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrices = np.array([squareform(pdist(combined[i], metric='cityblock')) for i in range(10)])\n",
    "distance_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.4700732 , 25.48903903, 25.53854045, 25.51355722, 25.31809289,\n",
       "       25.44363946, 25.56295549, 25.67546666, 25.58864391, 25.60535183])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_statistics = np.array([chamfer_L1_distance(distance_matrix, indices_list) for distance_matrix in distance_matrices])\n",
    "observed_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1506)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_lists = np.array([np.random.permutation(len_combined) for _ in range(9999)])\n",
    "index_lists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 1506)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_combined = distance_matrices[0].shape[0]\n",
    "len_pattern = index_lists.shape[1] // 2\n",
    "len_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9999, 1, 1), (9999, 753, 1), (9999, 1, 753))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx1 = np.arange(index_lists.shape[0])[:, None, None]\n",
    "idx2 = index_lists[:, :len_pattern][:, :, None]\n",
    "idx3 = index_lists[:, len_pattern:][:, None, :]\n",
    "idx1.shape, idx2.shape, idx3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m distances_1_to_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(\u001b[43mdistance_matrices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx3\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m distances_1_to_2\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "distances_1_to_2 = np.min(distance_matrices[0][idx1, idx2, idx3], axis=2)\n",
    "distances_1_to_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pattern = index_lists.shape[1] // 2\n",
    "indices_pattern = index_lists[:, :len_pattern]\n",
    "indices_control = index_lists[:, len_pattern:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9999, 753), (9999, 753))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indices_pattern.shape, indices_control.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1506, 1506), (10, 1506, 1506))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = distance_matrices[0]\n",
    "distance_matrix.shape, distance_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 825. TiB for an array with shape (9999, 753, 1, 9999, 1506) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistance_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_control\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 825. TiB for an array with shape (9999, 753, 1, 9999, 1506) and data type float64"
     ]
    }
   ],
   "source": [
    "distance_matrix[np.newaxis, :, :][:, indices_pattern, :][:, :, indices_control].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT suggestions for the batch calculation of chamfer distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of permutations.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 2D numpy array representing the pairwise distance matrix.\n",
    "        index_lists (ndarray): A 2D numpy array where each row represents a permutation of indices.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for each permutation.\n",
    "    \"\"\"\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "    indices_pattern = index_lists[:, :len_pattern]\n",
    "    indices_control = index_lists[:, len_pattern:]\n",
    "\n",
    "    # Broadcasting the distance matrix to match the dimensions required for batch processing\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.newaxis, :, :][:, indices_pattern, :][:, :, indices_control], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.newaxis, :, :][:, indices_control, :][:, :, indices_pattern], axis=2)\n",
    "\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of permutations.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 2D numpy array representing the pairwise distance matrix.\n",
    "        index_lists (ndarray): A 2D numpy array where each row represents a permutation of indices.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for each permutation.\n",
    "    \"\"\"\n",
    "    len_combined = distance_matrix.shape[0]\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "\n",
    "    # Create a 3D index array for pattern to control distances\n",
    "    idx1 = np.arange(index_lists.shape[0])[:, None, None]\n",
    "    idx2 = index_lists[:, :len_pattern][:, :, None]\n",
    "    idx3 = index_lists[:, len_pattern:][:, None, :]\n",
    "    \n",
    "    distances_1_to_2 = np.min(distance_matrix[idx1, idx2, idx3], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrix[idx1, idx3, idx2], axis=2)\n",
    "    \n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n",
      "(10, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "distance_matrices = np.zeros((10, 100, 100))\n",
    "x = np.arange(100)\n",
    "n_permutations = 9\n",
    "len_pattern = 50\n",
    "rng = np.random.default_rng(1234)\n",
    "indices_lists = rng.permuted(np.tile(x, n_permutations).reshape(n_permutations, x.size), axis=1)\n",
    "for index_list in indices_lists:\n",
    "    print(distance_matrices[np.ix_(np.arange(10), index_list[:len_pattern], index_list[len_pattern:])].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_lists[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "def chamfer_L1_distance_batch(distance_matrices, index_list):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of distance matrices\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 3D numpy array representing the pairwise distance matrix for many samples.\n",
    "        index_lists (ndarray): A 1D numpy array represents a permutation of indices.\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for all samples.\n",
    "    \"\"\"\n",
    "    len_pattern = index_list.shape[0] // 2\n",
    "    # Create a 3D index array for pattern to control distances\n",
    "    idx2 = index_list[:len_pattern]\n",
    "    idx3 = index_list[len_pattern:]\n",
    "    distances_1_to_2 = np.min(distance_matrices[np.ix_(np.arange(10), index_list[:len_pattern], index_list[len_pattern:])], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrices[np.ix_(np.arange(10), index_list[:len_pattern], index_list[len_pattern:])], axis=2)\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "distance_matrices = np.zeros((10, 100, 100))\n",
    "x = np.arange(100)\n",
    "n_permutations = 9\n",
    "len_pattern = 50\n",
    "rng = np.random.default_rng(1234)\n",
    "indices_lists = rng.permuted(np.tile(x, n_permutations).reshape(n_permutations, x.size), axis=1)\n",
    "for index_list in indices_lists:\n",
    "    print(distance_matrices[np.ix_(np.arange(10), index_list[:len_pattern], index_list[len_pattern:])].shape)\n",
    "    print(chamfer_L1_distance_batch(distance_matrices, index_list).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n",
      "(10, 50, 50)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "distance_matrices = np.zeros((10, 100, 100))\n",
    "x = np.arange(100)\n",
    "n_permutations = 9\n",
    "len_pattern = 50\n",
    "rng = np.random.default_rng(1234)\n",
    "indices_lists = rng.permuted(np.tile(x, n_permutations).reshape(n_permutations, x.size), axis=1)\n",
    "for index_list in indices_lists:\n",
    "    print(distance_matrices[np.ix_(np.arange(10), index_list[:len_pattern], index_list[len_pattern:])].shape)\n",
    "    print(chamfer_L1_distance_batch(distance_matrices, index_list).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def compute_power_permutation_test(params):\n",
    "    logging.info('Starting compute_power_permutation with params: %s', params)\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "        \n",
    "        # Generate 1000 samples of pattern and control\n",
    "        patterns = np.array([subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "        controls = np.array([subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "        logging.info('Generated the 1000 samples of pattern and control')\n",
    "\n",
    "        # Count max number of permutations with Combination rule nCr, where r is the pattern size\n",
    "        if sample < 15:\n",
    "            total_permutations = comb(sample*2, sample) # built in implementation of nCr rule.\n",
    "            # Adjust n_permutations if it's larger than total_permutations\n",
    "            if n_permutations > total_permutations:\n",
    "                exact_test = True\n",
    "                n_permutations = int(total_permutations)\n",
    "            else:\n",
    "                n_permutations = 9999\n",
    "                exact_test = False\n",
    "        else:\n",
    "            # If num_pattern is 15, the total combinations are 1.5e8, which already is much larger than 9999. So we skip calculating the factorials for 15+ to save compute time. \n",
    "            n_permutations = 9999\n",
    "            exact_test = False\n",
    "\n",
    "        pvalues = permutation_test_batch(patterns, controls, n_permutations=n_permutations, exact_test=exact_test)\n",
    "\n",
    "        critical_value = 0.05\n",
    "        adjusted_critical_value = critical_value / 5000\n",
    "        significant_count = np.sum(pvalues < critical_value)\n",
    "        bonferroni_count = np.sum(pvalues < adjusted_critical_value)\n",
    "\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count / 1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count / 1000)\n",
    "        \n",
    "        #with open('/media/gambino/students_workdir/nynke/blurry/results.txt', 'a') as f:\n",
    "        #    f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "        \n",
    "        logging.info('Finished compute_power_permutation with params: %s', params)\n",
    "        return result, bonferroni_result\n",
    "\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "def chamfer_L1_distance_batch(distance_matrices, index_list):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer L1 distances for a batch of distance matrices\n",
    "    Args:\n",
    "        distance_matrix (ndarray): A 3D numpy array representing the pairwise distance matrix for many samples.\n",
    "        index_lists (ndarray): A 1D numpy array represents a permutation of indices.\n",
    "    Returns:\n",
    "        ndarray: A 1D array of Chamfer L1 distances for all samples.\n",
    "    \"\"\"\n",
    "    len_pattern = index_list.shape[0] // 2\n",
    "    n_power_iterations=distance_matrices.shape[0]\n",
    "    # Create a 3D index array for pattern to control distances\n",
    "    idx2 = index_list[:len_pattern]\n",
    "    idx3 = index_list[len_pattern:]\n",
    "    distances_1_to_2 = np.min(distance_matrices[np.ix_(np.arange(n_power_iterations), idx2, idx3)], axis=2)\n",
    "    distances_2_to_1 = np.min(distance_matrices[np.ix_(np.arange(n_power_iterations), idx3, idx2)], axis=2)\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "\n",
    "def permutation_test_batch(patterns, controls, n_permutations=9999, exact_test=False):\n",
    "    combined = np.concatenate((patterns, controls), axis=1)\n",
    "    len_combined = combined.shape[1]\n",
    "    indices_list = np.arange(len_combined)\n",
    "\n",
    "    logging.info('Started permutation_test_batch')\n",
    "\n",
    "    distance_matrices = np.array([squareform(pdist(combined[i], metric='cityblock')) for i in range(1000)])\n",
    "    logging.info('computed all distance matrices')\n",
    "\n",
    "    # Compute observed statistics for all samples\n",
    "    #vectorized_chamfer_L1_distance = np.vectorize(chamfer_L1_distance, signature='(n,m)->()')\n",
    "    #observed_statistics = vectorized_chamfer_L1_distance(distance_matrices, indices_list)\n",
    "\n",
    "    observed_statistics = np.array([chamfer_L1_distance(distance_matrix, indices_list) for distance_matrix in distance_matrices])\n",
    "    logging.info('computed all observed statistics')\n",
    "    \n",
    "    # Generate index permutations\n",
    "    index_lists = np.array([np.random.permutation(len_combined) for _ in range(n_permutations)])\n",
    "    #vectorized_permutation = np.vectorize(np.random.permutation, signature='()->(n)')\n",
    "    #index_lists = vectorized_permutation(len_combined, size=n_permutations)\n",
    "\n",
    "    logging.info('Generated indices permutations')\n",
    "\n",
    "    # Function to compute Chamfer distances for a single permutation and sample. Reuse the index permutations for all 1000 power analysis runs.\n",
    "    #def compute_chamfer_distances(index_list):\n",
    "    #    return chamfer_L1_distance_batch(distance_matrices, index_lists)\n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples using multithreading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        chamfer_distances = np.array(list(executor.map(lambda index_list: chamfer_L1_distance_batch(distance_matrices, index_list), index_lists)))\n",
    "    \n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples\n",
    "    #chamfer_distances = np.array([chamfer_L1_distance_batch(distance_matrices, index_list) for index_list in index_lists])\n",
    "    logging.info('Computed all chamfer distances')\n",
    "\n",
    "    # Calculate p-values (one-sided test cause only interested in larger distances than H0)\n",
    "    eps = np.finfo(observed_statistics.dtype).eps * 100\n",
    "    gamma = np.abs(eps * observed_statistics)\n",
    "    cmps_greater = chamfer_distances >= observed_statistics[:, None] - gamma[:, None]\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum(axis=1) + adjustment) / (n_permutations + adjustment)\n",
    "    return pvalues_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_adata = subset_power_analysis(adata_test, mixed_patterns = True, pattern_strength= strength, rna_count = count, sample_size = sample, random_seed=False)\n",
    "control_adata = subset_power_analysis(adata_test, pattern = 'random', mixed_patterns = False, rna_count = count, sample_size = sample, random_seed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power_permutation(params):\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        # Given that random patterns have only 1442 cells simulated, we don't calculate the power for these above 1400 so that we don't need to sample with replacement\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "    \n",
    "        for i in range(1000):\n",
    "            # sample new gene. No random seed so that every time a different \"gene\" is sampled.\n",
    "            pattern = subset_power_analysis(adata_test, mixed_patterns = True, pattern_strength= strength, rna_count = count, sample_size = sample, random_seed=False)\n",
    "            control = subset_power_analysis(adata_test, pattern = 'random', mixed_patterns = False, rna_count = count, sample_size = sample, random_seed=False)\n",
    "\n",
    "            # Calculate null distribution and pvalue\n",
    "            pvalue = test_permutation(pattern.obsm[\"latent\"], control.obsm[\"latent\"], n_permutations=9999, return_distances = False)\n",
    "            \n",
    "            critical_value = 0.05\n",
    "            adjusted_critical_value = critical_value / 5000\n",
    "            if pvalue < critical_value:\n",
    "                significant_count += 1\n",
    "            if pvalue < adjusted_critical_value:\n",
    "                bonferroni_count += 1\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count/1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count/1000)\n",
    "\n",
    "        return result, bonferroni_result\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = ('low', '10-30', 753)\n",
    "result, bonferroni_result = compute_power_permutation(combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('low_10-30_753', 0.867)\n",
      "('low_10-30_753', 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(bonferroni_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='test.log', filemode='a', format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result, bonferroni_result \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_power_permutation_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(bonferroni_result)\n",
      "Cell \u001b[0;32mIn [60], line 32\u001b[0m, in \u001b[0;36mcompute_power_permutation_test\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     29\u001b[0m     n_permutations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9999\u001b[39m\n\u001b[1;32m     30\u001b[0m     exact_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m pvalues \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_test_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_permutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m critical_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     35\u001b[0m adjusted_critical_value \u001b[38;5;241m=\u001b[39m critical_value \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5000\u001b[39m\n",
      "Cell \u001b[0;32mIn [60], line 110\u001b[0m, in \u001b[0;36mpermutation_test_batch\u001b[0;34m(patterns, controls, n_permutations, exact_test)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Function to compute Chamfer distances for a single permutation and sample. Reuse the index permutations for all 1000 power analysis runs.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#def compute_chamfer_distances(index_list):\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#    return chamfer_L1_distance_batch(distance_matrices, index_lists)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Compute Chamfer distances for all permutations and samples using multithreading\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 110\u001b[0m     chamfer_distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchamfer_L1_distance_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_lists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compute Chamfer distances for all permutations and samples\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#chamfer_distances = np.array([chamfer_L1_distance_batch(distance_matrices, index_list) for index_list in index_lists])\u001b[39;00m\n\u001b[1;32m    115\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputed all chamfer distances\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/media/gambino/students_workdir/nynke/miniconda3/envs/blurry/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/media/gambino/students_workdir/nynke/miniconda3/envs/blurry/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/media/gambino/students_workdir/nynke/miniconda3/envs/blurry/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result, bonferroni_result = compute_power_permutation_test(combination)\n",
    "print(result)\n",
    "print(bonferroni_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1506)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_lists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1506)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_pattern = index_lists.shape[1] // 2\n",
    "index_lists[:, :len_pattern].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_pattern = index_lists[:, :len_pattern]\n",
    "indices_control = index_lists[:, len_pattern:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 753)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_pattern = index_lists.shape[1] // 2\n",
    "indices_pattern = index_lists[:, :len_pattern]\n",
    "indices_control = index_lists[:, len_pattern:]\n",
    "\n",
    "index_list = index_lists[0]\n",
    "np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 753)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_pattern.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix_3d = distance_matrix[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 1, 1506)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 753, 1)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_distance_matrices = distance_matrix_3d[indices_control, :, indices_pattern]\n",
    "subset_distance_matrices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 753)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(distance_matrix_3d[indices_control, :, indices_pattern], axis=2).shape\n",
    "I want my distance_matrix_3D to be of the dimensions (1506, 9999, 1506) instead of (1506,1,1506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D distance matrix using broadcasting\n",
    "distance_matrix_3d = distance_matrix[:, np.newaxis]\n",
    "\n",
    "# Subset the 3D distance matrix using advanced indexing\n",
    "subset_distance_matrices = distance_matrix_3d[indices_control, :, indices_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 1506)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506,)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute my chamfer distance for all permutations in one go. Idea is to broadcast the distance matrix to the shape (1506, 9999, 1506) and then subset it & get the nearest neighbours\n",
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "    indices_pattern = index_lists[:, :len_pattern]\n",
    "    indices_control = index_lists[:, len_pattern:]\n",
    "\n",
    "    distances_1_to_2 = np.min(distance_matrix[:, indices_pattern, indices_control], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[:, indices_control, indices_pattern], axis=1)\n",
    "\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [165], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indices_pattern \u001b[38;5;241m=\u001b[39m \u001b[43mindex_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mlen_pattern\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m indices_control \u001b[38;5;241m=\u001b[39m index_list[:, len_pattern:]\n\u001b[1;32m      3\u001b[0m distance_matrix[:, indices_pattern, indices_control]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "indices_pattern = index_list[:, :len_pattern]\n",
    "indices_control = index_list[:, len_pattern:]\n",
    "distance_matrix[:, indices_pattern, indices_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_pattern = index_lists.shape[1] // 2\n",
    "indices_pattern = index_lists[:, :len_pattern]\n",
    "indices_control = index_lists[:, len_pattern:]\n",
    "\n",
    "distances_1_to_2 = np.min(distance_matrix[:, indices_pattern, indices_control], axis=2)\n",
    "distances_2_to_1 = np.min(distance_matrix[:, indices_control, indices_pattern], axis=2)\n",
    "\n",
    "return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "def compute_power_permutation_test(params):\n",
    "    logging.info('Starting compute_power_permutation with params: %s', params)\n",
    "    try:\n",
    "        strength, count, sample = params\n",
    "        significant_count = 0\n",
    "        bonferroni_count = 0\n",
    "        if (count == '0-10' and sample > 1400) or (count == '10-30' and sample > 5100):\n",
    "            return (f'{strength}_{count}_{sample}', -1)\n",
    "\n",
    "        # Generate 1000 samples of pattern and control\n",
    "        patterns = np.array([subset_power_analysis(adata_test, mixed_patterns=True, pattern_strength=strength, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "        controls = np.array([subset_power_analysis(adata_test, pattern='random', mixed_patterns=False, rna_count=count, sample_size=sample, random_seed=False).obsm[\"latent\"] for _ in range(1000)])\n",
    "        logging.info('Generated the 1000 samples of pattern and control')\n",
    "        # Count max number of permutations with Combination rule nCr, where r is the pattern size\n",
    "        if sample < 15:\n",
    "            total_permutations = comb(sample*2, sample) # built in implementation of nCr rule.\n",
    "            # Adjust n_permutations if it's larger than total_permutations\n",
    "            if n_permutations > total_permutations:\n",
    "                exact_test = True\n",
    "                n_permutations = int(total_permutations)\n",
    "            else:\n",
    "                n_permutations = 9999\n",
    "                exact_test = False\n",
    "        else:\n",
    "            # If num_pattern is 15, the total combinations are 1.5e8, which already is much larger than 9999. So we skip calculating the factorials for 15+ to save compute time. \n",
    "            n_permutations = 9999\n",
    "            exact_test = False\n",
    "\n",
    "        pvalues = permutation_test_batch(patterns, controls, n_permutations=n_permutations, exact_test=exact_test)\n",
    "\n",
    "        critical_value = 0.05\n",
    "        adjusted_critical_value = critical_value / 5000\n",
    "        significant_count = np.sum(pvalues < critical_value)\n",
    "        bonferroni_count = np.sum(pvalues < adjusted_critical_value)\n",
    "\n",
    "        result = (f'{strength}_{count}_{sample}', significant_count / 1000)\n",
    "        bonferroni_result = (f'{strength}_{count}_{sample}', bonferroni_count / 1000)\n",
    "        \n",
    "        #with open('/media/gambino/students_workdir/nynke/blurry/results.txt', 'a') as f:\n",
    "        #    f.write(f'{strength}\\t{count}\\t{sample}\\t{significant_count / 1000}\\t{bonferroni_count / 1000}\\n')\n",
    "        \n",
    "        logging.info('Finished compute_power_permutation with params: %s', params)\n",
    "        return result, bonferroni_result\n",
    "\n",
    "    except ValueError as e:\n",
    "        if str(e) == \"Cannot take a larger sample than population when 'replace=False'\":\n",
    "            print(f\"Error for parameters: {params}\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def chamfer_L1_distance_batch(distance_matrix, index_lists):\n",
    "    len_pattern = index_lists.shape[1] // 2\n",
    "    indices_pattern = index_lists[:, :len_pattern]\n",
    "    indices_control = index_lists[:, len_pattern:]\n",
    "\n",
    "    distances_1_to_2 = np.min(distance_matrix[:, indices_pattern, indices_control], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[:, indices_control, indices_pattern], axis=1)\n",
    "\n",
    "    return np.mean(distances_1_to_2, axis=1) + np.mean(distances_2_to_1, axis=1)\n",
    "\n",
    "\n",
    "def permutation_test_batch(patterns, controls, n_permutations=9999, exact_test=False):\n",
    "    combined = np.concatenate((patterns, controls), axis=1)\n",
    "    len_combined = combined.shape[1]\n",
    "    indices_list = np.arange(len_combined)\n",
    "\n",
    "    logging.info('Started permutation_test_batch')\n",
    "\n",
    "    # Compute distance matrices for all combined samples\n",
    "    distance_matrices = np.array([squareform(pdist(combined[i], metric='cityblock')) for i in range(1000)])\n",
    "    logging.info('computed all distance matrices')\n",
    "\n",
    "    # Compute observed statistics for all samples\n",
    "    observed_statistics = np.array([chamfer_L1_distance(distance_matrix, indices_list) for distance_matrix in distance_matrices])\n",
    "    logging.info('computed all observed statistics')\n",
    "    \n",
    "    # Generate index permutations\n",
    "    index_lists = np.array([np.random.permutation(len_combined) for _ in range(n_permutations)])\n",
    "    logging.info('Generated indices permutations')\n",
    "\n",
    "    # Function to compute Chamfer distances for a single permutation and sample. Reuse the index permutations for all 1000 power analysis runs.\n",
    "    def compute_chamfer_distances(distance_matrix):\n",
    "        return chamfer_L1_distance_batch(distance_matrix, index_lists)\n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples using multithreading\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        chamfer_distances = np.array(list(executor.map(compute_chamfer_distances, distance_matrices)))\n",
    "    logging.info('Computed all chamfer distances')\n",
    "\n",
    "    # Compute Chamfer distances for all permutations and samples\n",
    "    #chamfer_distances = np.array([chamfer_L1_distance_batch(distance_matrix, index_lists) for distance_matrix in distance_matrices])\n",
    "\n",
    "    # Calculate p-values (one-sided test cause only interested in larger distances than H0)\n",
    "    eps = np.finfo(observed_statistics.dtype).eps * 100\n",
    "    gamma = np.abs(eps * observed_statistics)\n",
    "    cmps_greater = chamfer_distances >= observed_statistics[:, None] - gamma[:, None]\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum(axis=1) + adjustment) / (n_permutations + adjustment)\n",
    "    return pvalues_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = pattern_adata.obsm['latent']\n",
    "control = control_adata.obsm['latent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate([pattern, control])\n",
    "len_combined = len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 1506)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix = squareform(pdist(combined, metric='cityblock'))\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lists = np.apply_along_axis(np.random.permutation, 1, np.tile(list(range(len_combined)), (n_permutations, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_distances = np.array([chamfer_L1_distance_test(distance_matrix, index_list) for index_list in index_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_distances = np.array([chamfer_L1_distance_test(distance_matrix, index_list) for index_list in index_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_permutation(pattern, control, n_permutations = 9999, return_distances = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_test(pattern, control, n_permutations = 9999, return_distances = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 188, 1319,  170, ...,  550,  136, 1134])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import comb\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    distances_1_to_2 = np.min(distance_matrix[np.ix_(index_list[:len_pattern], index_list[len_pattern:])], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "def test_permutation(pattern, control, n_permutations: int = 9999, return_distances: bool = False):\n",
    "    combined = np.concatenate([pattern, control])\n",
    "    distance_matrix = squareform(pdist(combined, metric='cityblock'))\n",
    "    len_combined = len(combined)\n",
    "    num_pattern = len(pattern)\n",
    "    observed_statistic = chamfer_L1_distance(distance_matrix, list(range(len_combined)))\n",
    "\n",
    "    if num_pattern < 15:\n",
    "        total_permutations = comb(len_combined, num_pattern)\n",
    "        if n_permutations > total_permutations:\n",
    "            exact_test = True\n",
    "            n_permutations = int(total_permutations)\n",
    "        else:\n",
    "            exact_test = False\n",
    "    else:\n",
    "        exact_test = False\n",
    "\n",
    "    index_lists = np.apply_along_axis(np.random.permutation, 1, np.tile(list(range(len_combined)), (n_permutations, 1)))\n",
    "\n",
    "    chamfer_distances = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(chamfer_L1_distance, distance_matrix, index_list) for index_list in index_lists]\n",
    "        for future in as_completed(futures):\n",
    "            chamfer_distances.append(future.result())\n",
    "\n",
    "    chamfer_distances = np.array(chamfer_distances)\n",
    "\n",
    "    eps = (0 if not np.issubdtype(observed_statistic.dtype, np.inexact)\n",
    "           else np.finfo(observed_statistic.dtype).eps * 100)\n",
    "    gamma = np.abs(eps * observed_statistic)\n",
    "    cmps_greater = chamfer_distances >= observed_statistic - gamma\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum() + adjustment) / (n_permutations + adjustment)\n",
    "    p_value = pvalues_greater\n",
    "\n",
    "    if return_distances:\n",
    "        return p_value, observed_statistic, chamfer_distances\n",
    "    else:\n",
    "        return p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0147"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_test(pattern, control, n_permutations = 9999, return_distances = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 150,  678,   94, ...,  423, 1409,  203],\n",
       "       [  21, 1303, 1187, ..., 1477, 1179, 1031],\n",
       "       [1180, 1205, 1378, ...,  698,  328, 1013],\n",
       "       ...,\n",
       "       [ 521,  511, 1279, ..., 1472,  801,  105],\n",
       "       [ 841,  725,  301, ...,  173, 1204,  174],\n",
       "       [  23, 1043,  774, ...,  952,  147,  840]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(np.random.permutation, 1, np.tile(index_list, (10, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    " #compute distance matrix of combined (pdist) only once (combined 2000x2000)\n",
    "    #generate a list 1000x(len(array)) indices (ask chatgpt for vectorized way)\n",
    "    #for all indices combos in the list (will run 1000 times):    \n",
    "        # an indices of 1000 for pattern and 1000 for control\n",
    "        # change chamfer_L1_distance_cpu(permuted_pattern, permuted_control) to\n",
    "        # chamfer_L1_distance_cpu(pdist, indices) and inside we will filter distances 1 to 2 and distances 2 to 1\n",
    "        # and continue chamfer as before\n",
    "\n",
    "def chamfer_L1_distance(distance_matrix, index_list):\n",
    "    len_pattern = len(index_list) // 2\n",
    "    # subset the distance matrix with the indices of both point clouds, and get the nearest neighbor for each point from point cloud 1 in point cloud 2 and vice versa\n",
    "    distances_1_to_2 = np.min(distance_matrix[index_list[:len_pattern]][index_list[len_pattern:]], axis=1)\n",
    "    distances_2_to_1 = np.min(distance_matrix[np.ix_(index_list[len_pattern:], index_list[:len_pattern])], axis=1)\n",
    "\n",
    "    # Compute the Chamfer distance\n",
    "    return np.mean(distances_1_to_2) + np.mean(distances_2_to_1)\n",
    "\n",
    "\n",
    "def permutation_test(pattern, control, n_permutations: int = 9999, return_distances: bool = False):\n",
    "    combined = np.concatenate([pattern, control])\n",
    "    distance_matrix = squareform(pdist(combined, metric='cityblock'))\n",
    "    len_combined = len(combined)\n",
    "    num_pattern = len(pattern)\n",
    "    observed_statistic = chamfer_L1_distance(distance_matrix, list(range(len_combined)))\n",
    "\n",
    "    # Count max number of permutations with Combination rule nCr, where r is the pattern size\n",
    "    if num_pattern < 15:\n",
    "        total_permutations = comb(len_combined, num_pattern) # built in implementation of nCr rule.\n",
    "\n",
    "        # Adjust n_permutations if it's larger than total_permutations\n",
    "        if n_permutations > total_permutations:\n",
    "            exact_test = True\n",
    "            n_permutations = int(total_permutations)\n",
    "        else:\n",
    "            exact_test = False\n",
    "    else:\n",
    "        # If num_pattern is 15, the total combinations are 1.5e8, which already is much larger than 9999. So we skip calculating the factorials for 15+ to save compute time. \n",
    "        exact_test = False\n",
    "\n",
    "    # Permute the indices of the combined array n_permutations times and calculate the Chamfer distance for each permutation\n",
    "    index_lists = np.apply_along_axis(np.random.permutation, 1, np.tile(list(range(len_combined)), (n_permutations, 1)))\n",
    "    chamfer_distances = np.array([chamfer_L1_distance(distance_matrix, index_list) for index_list in index_lists])\n",
    "    \n",
    "    #These functions come from scipy.stats.permutation_test(). They have now been integrated in my main function in line to improve the efficiency\n",
    "    eps =  (0 if not np.issubdtype(observed_statistic.dtype, np.inexact)\n",
    "        else np.finfo(observed_statistic.dtype).eps*100)\n",
    "    gamma = np.abs(eps * observed_statistic)\n",
    "    cmps_greater = chamfer_distances >= observed_statistic - gamma\n",
    "    # +1 is added to pvalues to add the observed value into the hypothetical population to make the pvalue more conservative. If it is an exact test, will use the true pvalue.\n",
    "    adjustment = 0 if exact_test else 1\n",
    "    pvalues_greater = (cmps_greater.sum() + adjustment) / (n_permutations + adjustment)\n",
    "    # I do a 1-tailed test because I only care if the observed statistic has a larger chamfer distance than the H0 population.\n",
    "    p_value = pvalues_greater\n",
    "    \n",
    "    if return_distances == True:\n",
    "        return p_value, observed_statistic, chamfer_distances\n",
    "    else:  \n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue, observed_statistic, permuted_statistics = permutation_test_cpu(pattern.obsm[\"latent\"], control.obsm[\"latent\"], n_permutations=9999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blurry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
